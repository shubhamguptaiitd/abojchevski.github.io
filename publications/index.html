<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Aleksandar  Bojchevski | Publications</title>
<meta name="description" content="Faculty @ CISPA Helmholtz Center for Information Security">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Theming-->


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Aleksandar</span>   Bojchevski
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              Home
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/positions/">
                Open Positions
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description">publications in reversed chronological order </br> * denotes equal contribution</p>
  </header>

  <article>
    <div class="publications">
<!-- * denotes equal contribution -->
<!-- <h1> preprints </h1> -->

<p>An up-to-date list is available on <a href="https://scholar.google.com/citations?user=F1APiN4AAAAJ" target="_blank">Google Scholar</a>.</p>

<h1> conferences &amp; journals </h1>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="geisler2021robustness" class="col-sm-8 anchor">
    
      <div class="title">Robustness of Graph Neural Networks at Scale</div>
      <div class="author">
        
          
            
              
                
                  Simon Geisler,
                
              
            
          
        
          
            
              
                
                  Thomas Schmidt,
                
              
            
          
        
          
            
              
                
                  Hakan Şirin,
                
              
            
          
        
          
            
              
                
                  Daniel Zügner,
                
              
            
          
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Neural Information Processing Systems, NeurIPS</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2110.14038" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/sigeisler/robustness_of_gnns_at_scale" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    <a href="https://recorder-v3.slideslive.com/#/share?share=52006&amp;s=dda3ee8e-b5dd-40a3-b7f8-37eff971aed4" class="btn btn-sm z-depth-0" role="button" target="_blank">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Graph Neural Networks (GNNs) are increasingly important given their popularity and the diversity of applications. Yet, existing studies of their vulnerability to adversarial attacks rely on relatively small graphs. We address this gap and study how to attack and defend GNNs at scale. We propose two sparsity-aware first-order optimization attacks that maintain an efficient representation despite optimizing over a number of parameters which is quadratic in the number of nodes. We show that common surrogate losses are not well-suited for global attacks on GNNs. Our alternatives can double the attack strength. Moreover, to improve GNNs’ reliability we design a robust aggregation function, Soft Median, resulting in an effective defense at all scales. We evaluate our attacks and defense with standard GNNs on graphs more than 100 times larger compared to previous work. We even scale one order of magnitude further by extending our techniques to a scalable GNN.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="schuchardt2021collective" class="col-sm-8 anchor">
    
      <div class="title">Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks</div>
      <div class="author">
        
          
            
              
                
                  Jan Schuchardt,
                
              
            
          
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  Johannes Klicpera,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations, ICLR</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openreview.net/forum?id=ULQdiUTHe3y" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://openreview.net/pdf?id=ULQdiUTHe3y" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/jan-schuchardt/collective_robustness" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_collective.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
      
      <a href="/assets/pdf/slides_collective.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    <a href="https://www.youtube.com/watch?v=TDd3tRZGCI0" class="btn btn-sm z-depth-0" role="button" target="_blank">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In tasks like node classification, image segmentation, and named-entity recognition we have a classifier that simultaneously outputs multiple predictions (a vector of labels) based on a single input, i.e. a single graph, image, or document respectively. Existing adversarial robustness certificates consider each prediction independently and are thus overly pessimistic for such tasks. They implicitly assume that an adversary can use different perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input. We propose the first collective robustness certificate which computes the number of predictions that are simultaneously guaranteed to remain stable under perturbation, i.e. cannot be attacked. We focus on Graph Neural Networks and leverage their locality property - perturbations only affect the predictions in a close neighborhood - to fuse multiple single-node certificates into a drastically stronger collective certificate. For example, on the Citeseer dataset our collective certificate for node classification increases the average number of certifiable feature perturbations from 7 to 351.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AISTATS</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="wu2021completing" class="col-sm-8 anchor">
    
      <div class="title">Completing the Picture: Randomized Smoothing Suffers from the Curse of Dimensionality for a Large Family of Distributions</div>
      <div class="author">
        
          
            
              
                
                  Yihan Wu,
                
              
            
          
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  Aleksei Kuvshinov,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Artificial Intelligence and Statistics, AISTATS</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="http://proceedings.mlr.press/v130/wu21d/wu21d.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/YihanWu95/smoothing" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_curse.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    <a href="https://virtual.aistats.org/virtual/2021/poster/1967" class="btn btn-sm z-depth-0" role="button" target="_blank">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Randomized smoothing is currently the most competitive technique for providing provable robustness guarantees. Since this approach is model-agnostic and inherently scalable we can certify arbitrary classifiers. Despite its success, recent works show that for a small class of i.i.d. distributions, the largest lp radius that can be certified using randomized smoothing decreases as O(1/d^(1/2-1/p)) with dimension d for p &gt; 2. We complete the picture and show that similar no-go results hold for the l2 norm for a much more general family of distributions which are continuous and symmetric about the origin. Specifically, we calculate two different upper bounds of the l2 certified radius which have a constant multiplier of order Theta(1/d^1/2). Moreover, we extend our results to lp (p&gt;2) certification with spherical symmetric distributions solidifying the limitations of randomized smoothing. We discuss the implications of our results for how accuracy and robustness are related, and why robust training with noise augmentation can alleviate some of the limitations in practice. We also show that on real-world data the gap between the certified radius and our upper bounds is small.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="bojchevski2020sparse" class="col-sm-8 anchor">
    
      <div class="title">Efficient Robustness Certificates for Discrete Data: Sparsity-Aware Randomized Smoothing for Graphs, Images and More</div>
      <div class="author">
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  Johannes Klicpera,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning, ICML</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2008.12952" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/abojchevski/sparse_smoothing" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
      
      <a href="/assets/pdf/slides_sparse_smoothing.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    <a href="https://icml.cc/virtual/2020/poster/6848" class="btn btn-sm z-depth-0" role="button" target="_blank">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Existing techniques for certifying the robustness of models for discrete data either work only for a small class of models or are general at the expense of efficiency or tightness. Moreover, they do not account for sparsity in the input which, as our findings show, is often essential for obtaining non-trivial guarantees. We propose a model-agnostic certificate based on the randomized smoothing framework which subsumes earlier work and is tight, efficient, and sparsity-aware. Its computational complexity does not depend on the number of discrete categories or the dimension of the input (e.g. the graph size), making it highly scalable. We show the effectiveness of our approach on a wide variety of models, datasets, and tasks – specifically highlighting its use for Graph Neural Networks. So far, obtaining provable guarantees for GNNs has been difficult due to the discrete and non-i.i.d. nature of graph data. Our method can certify any GNN and handles perturbations to both the graph structure and the node attributes.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">KDD</abbr>
    
  
  
  <span class="award badge">Oral</span>
  
  <!--  -->
  </div>

  <div id="bojchevski2020pprgo" class="col-sm-8 anchor">
    
      <div class="title">Scaling Graph Neural Networks with Approximate PageRank</div>
      <div class="author">
        
          
            
              
                <em>Aleksandar Bojchevski*</em>,
              
            
          
        
          
            
              
                
                  Johannes Klicpera*,
                
              
            
          
        
          
            
              
                
                  Bryan Perozzi,
                
              
            
          
        
          
            
              
                
                  Amol Kapoor,
                
              
            
          
        
          
            
              
                
                  Martin Blais,
                
              
            
          
        
          
            
              
                
                  Benedek Rózemberczki,
                
              
            
          
        
          
            
              
                
                  Michal Lukasik,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Knowledge Discovery and Data Mining, KDD</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2007.01570" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/TUM-DAML/pprgo_pytorch" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
      
      <a href="/assets/pdf/slides_pprgo.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    <a href="https://www.youtube.com/watch?v=nsngoAycymE" class="btn btn-sm z-depth-0" role="button" target="_blank">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Graph neural networks (GNNs) have emerged as a powerful approach for solving many network mining tasks. However, learning on large graphs remains a challenge - many recently proposed scalable GNN approaches rely on an expensive message-passing procedure to propagate information through the graph. We present the PPRGo model which utilizes an efficient approximation of information diffusion in GNNs resulting in significant speed gains while maintaining state-of-the-art prediction performance. In addition to being faster, PPRGo is inherently scalable, and can be trivially parallelized for large datasets like those found in industry settings. We demonstrate that PPRGo outperforms baselines in both distributed and single-machine training environments on a number of commonly used academic graphs. To better analyze the scalability of large-scale graph learning methods, we introduce a novel benchmark graph with 12.4 million nodes, 173 million edges, and 2.8 million node features. We show that training PPRGo from scratch and predicting labels for all nodes in this graph takes under 2 minutes on a single machine, far outpacing other baselines on the same graph. We discuss the practical application of PPRGo to solve large-scale node classification problems at Google.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ALENEX</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="angriman2020group" class="col-sm-8 anchor">
    
      <div class="title">Group Centrality Maximization for Large-scale Graphs</div>
      <div class="author">
        
          
            
              
                
                  Eugenio Angriman,
                
              
            
          
        
          
            
              
                
                  Alexander Grinten,
                
              
            
          
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  Daniel Zügner,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>,
                
              
            
          
        
          
            
              
                
                  and Henning Meyerhenke
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Symposium on Algorithm Engineering and Experiments, ALENEX</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1910.13874" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://networkit.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The study of vertex centrality measures is a key aspect of network analysis. Naturally, such centrality measures have been generalized to groups of vertices; for popular measures it was shown that the problem of finding the most central group is NP-hard. As a result, approximation algorithms to maximize group centralities were introduced recently. Despite a nearly-linear running time, approximation algorithms for group betweenness and (to a lesser extent) group closeness are rather slow on large networks due to high constant overheads. That is why we introduce GED-Walk centrality, a new submodular group centrality measure inspired by Katz centrality. In contrast to closeness and betweenness, it considers walks of any length rather than shortest paths, with shorter walks having a higher contribution. We define algorithms that (i) efficiently approximate the GED-Walk score of a given group and (ii) efficiently approximate the (proved to be NP-hard) problem of finding a group with highest GED-Walk score. Experiments on several real-world datasets show that scores obtained by GED-Walk improve performance on common graph mining tasks such as collective classification and graph-level classification. An evaluation of empirical running times demonstrates that maximizing GED-Walk (in approximation) is two orders of magnitude faster compared to group betweenness approximation and for group sizes ≤100 one to two orders faster than group closeness approximation. For graphs with tens of millions of edges, approximate GED-Walk maximization typically needs less than one minute. Furthermore, our experiments suggest that the maximization algorithms scale linearly with the size of the input graph and the size of the group.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="bojchevski19certifiable" class="col-sm-8 anchor">
    
      <div class="title">Certifiable Robustness to Graph Perturbations</div>
      <div class="author">
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Neural Information Processing Systems, NeurIPS</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1910.14356" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/abojchevski/graph_cert" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_graph_cert.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness. This is even more alarming given recent findings showing that they are extremely vulnerable to adversarial attacks on both the graph structure and the node attributes. We propose the first method for verifying certifiable (non-)robustness to graph perturbations for a general class of models that includes graph neural networks and label/feature propagation. By exploiting connections to PageRank and Markov decision processes our certificates can be efficiently (and under many threat models exactly) computed. Furthermore, we investigate robust training procedures that increase the number of certifiably robust nodes while maintaining or improving the clean predictive accuracy.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  
  <span class="award badge">Oral</span>
  
  <!--  -->
  </div>

  <div id="bojchevski2019adversarial" class="col-sm-8 anchor">
    
      <div class="title">Adversarial Attacks on Node Embeddings via Graph Poisoning</div>
      <div class="author">
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning, ICML</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1809.01093" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/abojchevski/node_embedding_attack" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_embedding_attack.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
      
      <a href="/assets/pdf/slides_embedding_attack.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    <a href="https://www.videoken.com/embed/1zMVZKlxfU4?tocitem=1" class="btn btn-sm z-depth-0" role="button" target="_blank">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for solving downstream tasks. However, despite the proliferation of such methods, there is currently no study of their robustness to adversarial attacks. We provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial perturbations that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="klicpera19predict" class="col-sm-8 anchor">
    
      <div class="title">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</div>
      <div class="author">
        
          
            
              
                
                  Johannes Klicpera,
                
              
            
          
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations, ICLR</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1810.05997" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/klicperajo/ppnp" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_ppnp.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model’s training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  
  <span class="award badge">Oral</span>
  
  <!--  -->
  </div>

  <div id="bojchevski2018netgan" class="col-sm-8 anchor">
    
      <div class="title">NetGAN: Generating Graphs via Random Walks</div>
      <div class="author">
        
          
            
              
                <em>Aleksandar Bojchevski*</em>,
              
            
          
        
          
            
              
                
                  Oleksandr Shchur*,
                
              
            
          
        
          
            
              
                
                  Daniel Zügner*,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning, ICML</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1803.00816" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/danielzuegner/netgan" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_netgan.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
      
      <a href="/assets/pdf/slides_netgan.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose NetGAN – the first implicit generative model for graphs able to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over the input graph. The proposed model is based on a stochastic neural network that generates discrete output samples and is trained using the Wasserstein GAN objective. NetGAN is able to produce graphs that exhibit well-known network patterns without explicitly specifying them in the model definition. At the same time, our model exhibits strong generalization properties, as highlighted by its competitive link prediction performance, despite not being trained specifically for this task. Being the first approach to combine both of these desirable properties, NetGAN opens exciting avenues for further research.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="bojchevski2018deep" class="col-sm-8 anchor">
    
      <div class="title">Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking</div>
      <div class="author">
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations, ICLR</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1707.03815" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/abojchevski/graph2gauss" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_graph2gauss.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss – an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty – by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="bojchevski2018bayesian" class="col-sm-8 anchor">
    
      <div class="title">Bayesian Robust Attributed Graph Clustering: Joint Learning of Partial Anomalies and Group Structure</div>
      <div class="author">
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Conference on Artificial Intelligence, AAAI</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/paper_paican.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/abojchevski/paican" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_paican.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the problem of robust attributed graph clustering. In real data, the clustering structure is often obfuscated  due to anomalies or corruptions. While robust methods have been recently introduced that handle anomalies as part of the clustering process, they all fail to account for one core aspect: 	Since attributed graphs consist of two views (network structure and attributes) anomalies might materialize only partially, i.e. instances might be corrupted in one view but perfectly fit in the other. In this case, we can still derive meaningful cluster assignments. Existing works only consider complete anomalies. In this paper, we present a novel probabilistic generative model (PAICAN) that explicitly models partial anomalies by generalizing ideas of Degree Corrected Stochastic Block Models and Bernoulli Mixture Models. We provide a highly scalable variational inference approach with runtime complexity linear in the number of edges. The robustness of our model w.r.t. anomalies is demonstrated by our experimental study, outperforming state-of-the-art competitors.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">BMC</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="cejuela2018loctext" class="col-sm-8 anchor">
    
      <div class="title">LocText: Relation Extraction of Protein Localizations to Assist Database Curation</div>
      <div class="author">
        
          
            
              
                
                  Juan Miguel Cejuela,
                
              
            
          
        
          
            
              
                
                  Shrikant Vinchurkar,
                
              
            
          
        
          
            
              
                
                  Tatyana Goldberg,
                
              
            
          
        
          
            
              
                
                  Madhukar S. Prabhu Shankar,
                
              
            
          
        
          
            
              
                
                  Ashish Baghudana,
                
              
            
          
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  Carsten Uhlig,
                
              
            
          
        
          
            
              
                
                  André Ofner,
                
              
            
          
        
          
            
              
                
                  Pandu Raharja-Liu,
                
              
            
          
        
          
            
              
                
                  Lars Juhl Jensen,
                
              
            
          
        
          
            
              
                
                  and  others
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>BMC Bioinformatics</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://pubmed.ncbi.nlm.nih.gov/29343218/" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
      <a href="https://github.com/Rostlab/LocText" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The subcellular localization of a protein is an important aspect of its function. However, the experimental annotation of locations is not even complete for well-studied model organisms. Text mining might aid database curators to add experimental annotations from the scientific literature. Existing extraction methods have difficulties to distinguish relationships between proteins and cellular locations co-mentioned in the same sentence. LocText was created as a new method to extract protein locations from abstracts and full texts. LocText learned patterns from syntax parse trees and was trained and evaluated on a newly improved LocTextCorpus. Combined with an automatic named-entity recognizer, LocText achieved high precision (P = 86%±4). After completing development, we mined the latest research publications for three organisms: human (Homo sapiens), budding yeast (Saccharomyces cerevisiae), and thale cress (Arabidopsis thaliana). Examining 60 novel, text-mined annotations, we found that 65% (human), 85% (yeast), and 80% (cress) were correct. Of all validated annotations, 40% were completely novel, i.e. did neither appear in the annotations nor the text descriptions of Swiss-Prot. LocText provides a cost-effective, semi-automated workflow to assist database curators in identifying novel protein localization annotations. The annotations suggested through text-mining would be verified by experts to guarantee high-quality standards of manually-curated databases such as Swiss-Prot.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">KDD</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="bojchevski2017robust" class="col-sm-8 anchor">
    
      <div class="title">Robust Spectral Clustering for Noisy Data: Modeling Sparse Corruptions Improves Latent Embeddings</div>
      <div class="author">
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  Yves Matkovic,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Knowledge Discovery and Data Mining, KDD</em>
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://dl.acm.org/doi/10.1145/3097983.3098156" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="/assets/pdf/paper_rsc.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/abojchevski/rsc" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_rsc.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Spectral clustering is one of the most prominent clustering approaches. However, it is highly sensitive to noisy input data. In this work, we propose a robust spectral clustering technique able to handle such scenarios. To achieve this goal, we propose a sparse and latent decomposition of the similarity graph used in spectral clustering. In our model, we jointly learn the spectral embedding as well as the corrupted data – thus, enhancing the clustering performance overall. We propose algorithmic solutions to all three established variants of spectral clustering, each showing linear complexity in the number of edges. Our experimental analysis confirms the significant potential of our approach for robust spectral clustering. Supplementary material is available at www.kdd.in.tum.de/RSC.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">BioInf</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="cejuela2017nala" class="col-sm-8 anchor">
    
      <div class="title">nala: Text Mining Natural Language Mutation Mentions</div>
      <div class="author">
        
          
            
              
                
                  Juan Miguel Cejuela,
                
              
            
          
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  Carsten Uhlig,
                
              
            
          
        
          
            
              
                
                  Rustem Bekmukhametov,
                
              
            
          
        
          
            
              
                
                  Sanjeev Kumar Karn,
                
              
            
          
        
          
            
              
                
                  Shpend Mahmuti,
                
              
            
          
        
          
            
              
                
                  Ashish Baghudana,
                
              
            
          
        
          
            
              
                
                  Ankit Dubey,
                
              
            
          
        
          
            
              
                
                  Venkata P Satagopam,
                
              
            
          
        
          
            
              
                
                  and Burkhard Rost
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Bioinformatics</em>
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://academic.oup.com/bioinformatics/article/33/12/1852/2991428" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
      <a href="https://github.com/Rostlab/nala" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The extraction of sequence variants from the literature remains an important task. Existing methods primarily target standard (ST) mutation mentions (e.g. ‘E6V’), leaving relevant mentions natural language (NL) largely untapped (e.g. ‘glutamic acid was substituted by valine at residue 6’). We introduced three new corpora suggesting named-entity recognition (NER) to be more challenging than anticipated: 28–77% of all articles contained mentions only available in NL. Our new method nala captured NL and ST by combining conditional random fields with word embedding features learned unsupervised from the entire PubMed. In our hands, nala substantially outperformed the state-of-the-art. For instance, we compared all unique mentions in new discoveries correctly detected by any of three methods (SETH, tmVar, or nala). Neither SETH nor tmVar discovered anything missed by nala, while nala uniquely tagged 33% mentions. For NL mentions the corresponding value shot up to 100% nala-only.</p>
    </div>
    
  </div>
</div>
</li></ol>


<h1> workshops </h1>
<ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MLG</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="bojchevski2019pagerank" class="col-sm-8 anchor">
    
      <div class="title">Is PageRank All You Need for Scalable Graph Neural Networks?</div>
      <div class="author">
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  Johannes Klicpera,
                
              
            
          
        
          
            
              
                
                  Bryan Perozzi,
                
              
            
          
        
          
            
              
                
                  Martin Blais,
                
              
            
          
        
          
            
              
                
                  Amol Kapoor,
                
              
            
          
        
          
            
              
                
                  Michal Lukasik,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Workshop on Mining and Learning with Graphs, MLG</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.mlgworkshop.org/2019/papers/MLG2019_paper_50.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/TUM-DAML/pprgo_pytorch" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Graph neural networks (GNNs) have emerged as a powerful approach for solving many network mining tasks. However, efficiently utilizing them on web-scale data remains a challenge despite related advances in research. Most recently proposed scalable GNNs rely on an expensive recursive message-passing procedure to propagate information through the graph. We circumvent this limitation by leveraging connections between GNNs and personalized PageRank and we develop a model that incorporates multi-hop neighborhood information in a single (non-recursive) step. Our work-in-progress approach PPRGo is significantly faster than multi-hop models while maintaining state-of-the-art prediction performance. We demonstrate the strengths and scalability of our approach on graphs orders of magnitude larger than typically considered in the literature.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">GEM</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="monti2018dual" class="col-sm-8 anchor">
    
      <div class="title">Dual-primal Graph Convolutional Networks</div>
      <div class="author">
        
          
            
              
                
                  Federico Monti,
                
              
            
          
        
          
            
              
                
                  Oleksandr Shchur,
                
              
            
          
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  Or Litany,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>,
                
              
            
          
        
          
            
              
                
                  and Michael M Bronstein
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Graph Embedding and Mining Workshop, GEM</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1806.00770" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In recent years, there has been a surge of interest in developing deep learning methods for non-Euclidean structured data such as graphs. In this paper, we propose Dual-Primal Graph CNN, a graph convolutional architecture that alternates convolution-like operations on the graph and its dual. Our approach allows to learn both vertex- and edge features and generalizes the previous graph attention (GAT) model. We provide extensive experimental validation showing state-of-the-art results on a variety of tasks tested on established graph benchmarks, including CORA and Citeseer citation networks as well as MovieLens, Flixter, Douban and Yahoo Music graph-guided recommender systems.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">R2L</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="shchur2018pitfalls" class="col-sm-8 anchor">
    
      <div class="title">Pitfalls of Graph Neural Network Evaluation</div>
      <div class="author">
        
          
            
              
                
                  Oleksandr Shchur,
                
              
            
          
        
          
            
              
                
                  Maximilian Mumme,
                
              
            
          
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Relational Representation Learning Workshop, R2L</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1811.05868" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/shchur/gnn-benchmark" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Semi-supervised node classification in graphs is a fundamental problem in graph mining, and the recently proposed graph neural networks (GNNs) have achieved unparalleled results on this task. Due to their massive success, GNNs have attracted a lot of attention, and many novel architectures have been put forward. In this paper we show that existing evaluation strategies for GNN models have serious shortcomings. We show that using the same train/validation/test splits of the same datasets, as well as making significant changes to the training procedure (e.g. early stopping criteria) precludes a fair comparison of different architectures. We perform a thorough empirical evaluation of four prominent GNN models and show that considering different splits of the data leads to dramatically different rankings of models. Even more importantly, our findings suggest that simpler GNN architectures are able to outperform the more sophisticated ones if the hyperparameters and the training procedure are tuned fairly for all models.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICDMW</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="shchur2018anomaly" class="col-sm-8 anchor">
    
      <div class="title">Anomaly Detection in Car-Booking Graphs</div>
      <div class="author">
        
          
            
              
                
                  Oleksandr Shchur,
                
              
            
          
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  Mohamed Farghal,
                
              
            
          
        
          
            
              
                
                  <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>,
                
              
            
          
        
          
            
              
                
                  and Yusuf Saber
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Data Mining Workshops, ICDM</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://ieeexplore.ieee.org/document/8637361" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The use of car-booking services has gained massive popularity in the recent years – which led to an increasing number of fraudsters that try to game these systems. In this paper we describe a framework for fraud detection in car-booking systems. Our core idea lies in casting this problem as an instance of anomaly detection in temporal graphs. Specifically, we use unsupervised techniques, such as dense subblock discovery, to detect suspicious activity. The proposed framework is able to adapt to the variations in the data inherent to the car-booking setting, and detects fraud with high precision. This work is performed in collaboration with Careem, where the described framework is currently being deployed in production.</p>
    </div>
    
  </div>
</div>
</li></ol>

<h1> theses </h1>
<ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">PhD</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="bojchevski2020thesis" class="col-sm-8 anchor">
    
    <div class="title">Machine Learning on Graphs in the Presence of Noise and Adversaries</div>
    <!-- <div class="author"> <em>Aleksandar Bojchevski</em> </div> -->
    <em> Technical University of Munich, 2020  </em>
      <!-- <span id="bojchevski2020thesis">Bojchevski, A. (2020). <i>Machine Learning on Graphs in the Presence of Noise and Adversaries</i>. Technical University of Munich.</span> -->
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>From protein interactions to social networks, complex systems of interlinked entities are endemic in a connected world and graphs are a powerful abstraction for capturing their structure. Accordingly, we have a rich literature of machine learning techniques for graph data to solve problems ranging from fraud detection to cancer classification. Since in reality data is unreliable understanding the robustness of these techniques to noise and adversaries is critical. The contributions of this thesis deepen our understanding of robustness for three types of models: unsupervised, generative, and semi-supervised.  First, we derive a noise-resilient variant of the classical spectral embedding, and we introduce Gaussian embeddings that represent nodes as distributions to capture uncertainty. Then we study the sensitivity of node embeddings to graph poisoning. Next, we develop a generative model that explicitly accounts for anomalies and detects clusters obfuscated by noise. Finally, we derive provable robustness guarantees. We propose the first certificate w.r.t. structure perturbations for a large class of PageRank-based models, and we derive a general certificate for discrete data applicable to any graph classifier.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MSc</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="bojchevski2016thesis" class="col-sm-8 anchor">
    
    <div class="title">Semi-supervised Learning for Biomedical Named-Entity Recognition</div>
    <!-- <div class="author"> <em>Aleksandar Bojchevski</em> </div> -->
    <em> Technical University of Munich, 2015  </em>
      <!-- <span id="bojchevski2016thesis">Bojchevski, A. (2015). <i>Semi-supervised Learning for Biomedical Named-Entity Recognition</i>. Technical University of Munich.</span> -->
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="/assets/pdf/master_thesis.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/Rostlab/nala" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
      
      <a href="/assets/pdf/master_thesis_slides.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As the volume of published research in the biomedical domain increases, the need for effective information extraction systems grows in parallel. In this context, the task of named-entity recognition (NER) is essential. NER is defined as the classification of words in free text that represent predefined categories such as genes, proteins or other entities. As a specific application of NER, the main focus of this thesis is the recognition of mutation mentions from the biomedical literature. More specifically we aim to create a model able to recognize mutation mentions expressed in natural language. The current state-of-the-art method, tmVar is only able to recognize a small subset of standard or semi-standard mentions. Our method both outperforms tmVar on those types of mentions and is also able to recognize natural language (NL) mentions. Previously no other method considered NL mutation mentions. The performance of NER machine learning models is intrinsically limited by the availability of high-quality-annotated corpora. The construction of such corpora is costly – specially when expert annotators are required. In the biomedical domain, the difficulty of the task is even greater, since the number of possible named entities is higher and keeps growing with new discoveries. To combat the lack of large annotated corpora, we turn to the exploitation of large volumes of unlabeled text, applying a semi-supervised learning approach. Using techniques for unsupervised feature learning we aim to increase the performance of traditional NER models. More specifically, this thesis focuses on augmenting common conditional random field (CRF) approaches combined with novel word representation features learned from large bodies of biomedical text. Furthermore, using an active learning approach we extend an existing corpus of mutation mentions (IDP4) with additional NL mentions. Finally, and in support of evaluating our semi-supervised learning approach, we develop a complete pipeline for biomedical named-entity recognition including preprocessing steps, feature generation, model learning and normalized predictions. Our extended corpus, NER tool and pipeline framework are all open sourced on GitHub.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">BEng</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="bojchevski2013" class="col-sm-8 anchor">
    
    <div class="title">Personality Prediction Based on Information from Social Networks</div>
    <!-- <div class="author"> <em>Aleksandar Bojchevski</em> </div> -->
    <em> Ss. Cyril and Methodius University, 2013  </em>
      <!-- <span id="bojchevski2013">Bojchevski, A. (2013). <i>Personality Prediction Based on Information from Social Networks</i>. Ss. Cyril and Methodius University.</span> -->
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2021 Aleksandar  Bojchevski.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    Last updated: December 06, 2021.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
