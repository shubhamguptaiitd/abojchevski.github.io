<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Aleksandar  Bojchevski</title>
<meta name="description" content="Faculty @ CISPA Helmholtz Center for Information Security">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Theming-->


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
   
  <a href="mailto:%6C%61%73%74%6E%61%6D%65%5B%61%74%5D%63%69%73%70%61.%64%65"><i class="fas fa-envelope"></i></a> 
  
  
  <a href="https://scholar.google.com/citations?user=F1APiN4AAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/abojchevski" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  
  <a href="https://twitter.com/abojchevski" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
  
</span>

        </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              Home
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/positions/">
                Open Positions
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Aleksandar</span>   Bojchevski
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
    </div>
    

    <div class="clearfix" align="justify">
      <p>Hi, I’m Aleks <img class="emoji" title=":wave:" alt=":wave:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44b.png" height="20" width="20"> I’m a tenure-track faculty at the <a href="https://cispa.de" target="_blank">CISPA Helmholtz Center for Information Security</a>. I’m broadly interested in trustworthy machine learning, 
that is models and algorithms that are not only accurate or efficient but also robust, privacy-preserving, fair, uncertainty-aware, interpretable, etc.
<!--  --></p>

<p>I completed my PhD on machine learning for graphs at the <a href="https://www.in.tum.de/daml/" target="_blank">DAML</a> group at TU Munich, advised by <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>.
<!-- During my PhD I interned at Google working with the graph mining team. -->
I have a MSc in computer science from TU Munich where I worked at <a href="https://www.rostlab.org/" target="_blank">Rostlab</a> on natural language mutation mentions.
<!-- Before that I studied at the Faculty of Computer Science and Engineering in Skopje, North Macedonia. --></p>

<p><strong>Research interests</strong>: adversarial robustness, provable guarantees, fairness, privacy-preserving machine learning, uncertainty estimation, interpretability, graph neural networks, graph representation learning, and (deep) generative models.
<!--  -->
If you are interested in working with us on these (or adjacent) topics don’t hesitate to get in touch.
We have multiple <a href="/positions">open positions</a>!</p>

    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Oct 21</th>
          <td>
            
              This semester I am teaching the <a href="https://cms.cispa.saarland/tgnn_ws21/" target="_blank">Trustworthy Graph Neural Networks</a> seminar.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 21</th>
          <td>
            
              Our <a href="/publications#geisler2021robustness">paper</a> on robustness of GNNs at scale was accepted at NeurIPS 2021.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 21</th>
          <td>
            
              I have joined <a href="https://cispa.de" target="_blank">CISPA</a> as a tenure-track faculty <img class="emoji" title=":microscope:" alt=":microscope:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f52c.png" height="20" width="20">. We have multiple open <a href="/positions">positions</a>!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 21</th>
          <td>
            
              I gave a talk on <em>Trustworthy Machine Learning for Graphs with Guarantees</em> at the <a href="https://www.ai.math.uni-muenchen.de/talks/index.html" target="_blank">Math of AI Seminar @ LMU</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Mar 21</th>
          <td>
            
              I gave a talk on <em>Trustworthy Machine Learning for Graphs</em> at <a href="https://cispa.de/en" target="_blank">CISPA</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Mar 21</th>
          <td>
            
              I gave a <a href="/assets/pdf/slides_talk_nec.pdf">talk</a> on <em>Provably Robust Machine Learning on Graphs</em> at <a href="https://www.neclab.eu/" target="_blank">NEC Labs Europe</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Feb 21</th>
          <td>
            
              Our <a href="/publications#wu2021completing">paper</a> on the curse of dimensionality for randomized smoothing <img class="emoji" title=":skull:" alt=":skull:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f480.png" height="20" width="20"> was accepted at AISTATS 2021.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jan 21</th>
          <td>
            
              Our <a href="/publications#schuchardt2021collective">paper</a> on collective robustness certificates was accepted at ICLR 2021.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 20</th>
          <td>
            
              I defended my PhD thesis with distinction (summa cum laude) <img class="emoji" title=":crossed_swords:" alt=":crossed_swords:" src="https://github.githubassets.com/images/icons/emoji/unicode/2694.png" height="20" width="20">.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 20</th>
          <td>
            
              Our <a href="/publications#bojchevski2020sparse">paper</a> on robustness certificates was accepted at ICML 2020.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications   <a href="/publications/"> [full list] </a> </h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="schuchardt2021collective" class="col-sm-8 anchor">
    
      <div class="title">Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks</div>
      <div class="author">
        
          
            
              
                
                  Jan Schuchardt,
                
              
            
          
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  Johannes Klicpera,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations, ICLR</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openreview.net/forum?id=ULQdiUTHe3y" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
      
      <a href="https://openreview.net/pdf?id=ULQdiUTHe3y" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/jan-schuchardt/collective_robustness" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_collective.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
      
      <a href="/assets/pdf/slides_collective.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    <a href="https://www.youtube.com/watch?v=TDd3tRZGCI0" class="btn btn-sm z-depth-0" role="button" target="_blank">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In tasks like node classification, image segmentation, and named-entity recognition we have a classifier that simultaneously outputs multiple predictions (a vector of labels) based on a single input, i.e. a single graph, image, or document respectively. Existing adversarial robustness certificates consider each prediction independently and are thus overly pessimistic for such tasks. They implicitly assume that an adversary can use different perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input. We propose the first collective robustness certificate which computes the number of predictions that are simultaneously guaranteed to remain stable under perturbation, i.e. cannot be attacked. We focus on Graph Neural Networks and leverage their locality property - perturbations only affect the predictions in a close neighborhood - to fuse multiple single-node certificates into a drastically stronger collective certificate. For example, on the Citeseer dataset our collective certificate for node classification increases the average number of certifiable feature perturbations from 7 to 351.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="bojchevski2020sparse" class="col-sm-8 anchor">
    
      <div class="title">Efficient Robustness Certificates for Discrete Data: Sparsity-Aware Randomized Smoothing for Graphs, Images and More</div>
      <div class="author">
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  Johannes Klicpera,
                
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning, ICML</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2008.12952" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/abojchevski/sparse_smoothing" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
      
      <a href="/assets/pdf/slides_sparse_smoothing.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    <a href="https://icml.cc/virtual/2020/poster/6848" class="btn btn-sm z-depth-0" role="button" target="_blank">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Existing techniques for certifying the robustness of models for discrete data either work only for a small class of models or are general at the expense of efficiency or tightness. Moreover, they do not account for sparsity in the input which, as our findings show, is often essential for obtaining non-trivial guarantees. We propose a model-agnostic certificate based on the randomized smoothing framework which subsumes earlier work and is tight, efficient, and sparsity-aware. Its computational complexity does not depend on the number of discrete categories or the dimension of the input (e.g. the graph size), making it highly scalable. We show the effectiveness of our approach on a wide variety of models, datasets, and tasks – specifically highlighting its use for Graph Neural Networks. So far, obtaining provable guarantees for GNNs has been difficult due to the discrete and non-i.i.d. nature of graph data. Our method can certify any GNN and handles perturbations to both the graph structure and the node attributes.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  
  <!--  -->
  </div>

  <div id="bojchevski19certifiable" class="col-sm-8 anchor">
    
      <div class="title">Certifiable Robustness to Graph Perturbations</div>
      <div class="author">
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Neural Information Processing Systems, NeurIPS</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1910.14356" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/abojchevski/graph_cert" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_graph_cert.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness. This is even more alarming given recent findings showing that they are extremely vulnerable to adversarial attacks on both the graph structure and the node attributes. We propose the first method for verifying certifiable (non-)robustness to graph perturbations for a general class of models that includes graph neural networks and label/feature propagation. By exploiting connections to PageRank and Markov decision processes our certificates can be efficiently (and under many threat models exactly) computed. Furthermore, we investigate robust training procedures that increase the number of certifiably robust nodes while maintaining or improving the clean predictive accuracy.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  
  <span class="award badge">Oral</span>
  
  <!--  -->
  </div>

  <div id="bojchevski2019adversarial" class="col-sm-8 anchor">
    
      <div class="title">Adversarial Attacks on Node Embeddings via Graph Poisoning</div>
      <div class="author">
        
          
            
              
                <em>Aleksandar Bojchevski</em>,
              
            
          
        
          
            
              
                
                  and <a href="https://www.in.tum.de/daml/team/guennemann/" target="_blank">Stephan Günnemann</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning, ICML</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1809.01093" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/abojchevski/node_embedding_attack" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/poster_embedding_attack.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
      
      <a href="/assets/pdf/slides_embedding_attack.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    <a href="https://www.videoken.com/embed/1zMVZKlxfU4?tocitem=1" class="btn btn-sm z-depth-0" role="button" target="_blank">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for solving downstream tasks. However, despite the proliferation of such methods, there is currently no study of their robustness to adversarial attacks. We provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial perturbations that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.</p>
    </div>
    
  </div>
</div>
</li>
</ol>
</div>

    

    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    © Copyright 2021 Aleksandar  Bojchevski.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
    Last updated: December 06, 2021.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
