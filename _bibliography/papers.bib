---
---
@inproceedings{geisler2021robustness,
author = {Geisler, Simon and Schmidt, Thomas, and {\c{S}}irin, Hakan and Z{\"u}gner, Daniel, and Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
title = {Robustness of Graph Neural Networks at Scale},
booktitle = {Neural Information Processing Systems, {NeurIPS}},
year      = {2021},
abbr      = {NeurIPS},
category  = {conference},
arxiv     = {2110.14038},
code      = {https://github.com/sigeisler/robustness_of_gnns_at_scale},
talk      = {https://recorder-v3.slideslive.com/#/share?share=52006&s=dda3ee8e-b5dd-40a3-b7f8-37eff971aed4},
abstract  = {Graph Neural Networks (GNNs) are increasingly important given their popularity and the diversity of applications. Yet, existing studies of their vulnerability to adversarial attacks rely on relatively small graphs. We address this gap and study how to attack and defend GNNs at scale. We propose two sparsity-aware first-order optimization attacks that maintain an efficient representation despite optimizing over a number of parameters which is quadratic in the number of nodes. We show that common surrogate losses are not well-suited for global attacks on GNNs. Our alternatives can double the attack strength. Moreover, to improve GNNs' reliability we design a robust aggregation function, Soft Median, resulting in an effective defense at all scales. We evaluate our attacks and defense with standard GNNs on graphs more than 100 times larger compared to previous work. We even scale one order of magnitude further by extending our techniques to a scalable GNN.},
}
@inproceedings{schuchardt2021collective,
author = {Schuchardt, Jan and Bojchevski, Aleksandar and Klicpera, Johannes and G{\"u}nnemann, Stephan},
title = {Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks},
booktitle = {International Conference on Learning Representations, {ICLR}},
year      = {2021},
code      = {https://github.com/jan-schuchardt/collective_robustness},
abbr      = {ICLR},
category  = {conference},
selected  = {true},
html      = {https://openreview.net/forum?id=ULQdiUTHe3y},
pdf       = {https://openreview.net/pdf?id=ULQdiUTHe3y},
slides    = {slides_collective.pdf},
poster    = {poster_collective.pdf},
talk      = {https://www.youtube.com/watch?v=TDd3tRZGCI0},
abstract  = {In tasks like node classification, image segmentation, and named-entity recognition we have a classifier that simultaneously outputs multiple predictions (a vector of labels) based on a single input, i.e. a single graph, image, or document respectively. Existing adversarial robustness certificates consider each prediction independently and are thus overly pessimistic for such tasks. They implicitly assume that an adversary can use different perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input. We propose the first collective robustness certificate which computes the number of predictions that are simultaneously guaranteed to remain stable under perturbation, i.e. cannot be attacked. We focus on Graph Neural Networks and leverage their locality property - perturbations only affect the predictions in a close neighborhood - to fuse multiple single-node certificates into a drastically stronger collective certificate. For example, on the Citeseer dataset our collective certificate for node classification increases the average number of certifiable feature perturbations from 7 to 351.},
}
@inproceedings{wu2021completing,
author    = {Wu, Yihan and Bojchevski, Aleksandar and Kuvshinov, Aleksei and G{\"u}nnemann, Stephan},
title     = {Completing the Picture: Randomized Smoothing Suffers from the Curse of Dimensionality for a Large Family of Distributions},
booktitle = {International Conference on Artificial Intelligence and Statistics, {AISTATS}},
year      = {2021},
category  = {conference},
abbr      = {AISTATS},
pdf       = {http://proceedings.mlr.press/v130/wu21d/wu21d.pdf},
code      = {https://github.com/YihanWu95/smoothing},
talk      = {https://virtual.aistats.org/virtual/2021/poster/1967},
poster    = {poster_curse.pdf},
abstract  = {Randomized smoothing is currently the most competitive technique for providing provable robustness guarantees. Since this approach is model-agnostic and inherently scalable we can certify arbitrary classifiers. Despite its success, recent works show that for a small class of i.i.d. distributions, the largest lp radius that can be certified using randomized smoothing decreases as $O(1/d^(1/2-1/p))$ with dimension d for p > 2. We complete the picture and show that similar no-go results hold for the l2 norm for a much more general family of distributions which are continuous and symmetric about the origin. Specifically, we calculate two different upper bounds of the l2 certified radius which have a constant multiplier of order Theta(1/d^1/2). Moreover, we extend our results to lp (p>2) certification with spherical symmetric distributions solidifying the limitations of randomized smoothing. We discuss the implications of our results for how accuracy and robustness are related, and why robust training with noise augmentation can alleviate some of the limitations in practice. We also show that on real-world data the gap between the certified radius and our upper bounds is small.},
}
@inproceedings{bojchevski2020sparse,
  title     = {Efficient Robustness Certificates for Discrete Data: Sparsity-Aware Randomized Smoothing for Graphs, Images and More},
  author    = {Bojchevski, Aleksandar and Klicpera, Johannes and G{\"u}nnemann, Stephan},
  booktitle = {International Conference on Machine Learning, {ICML}},
  year      = {2020},
  abstract  = {Existing techniques for certifying the robustness of models for discrete data either work only for a small class of models or are general at the expense of efficiency or tightness. Moreover, they do not account for sparsity in the input which, as our findings show, is often essential for obtaining non-trivial guarantees. We propose a model-agnostic certificate based on the randomized smoothing framework which subsumes earlier work and is tight, efficient, and sparsity-aware. Its computational complexity does not depend on the number of discrete categories or the dimension of the input (e.g. the graph size), making it highly scalable. We show the effectiveness of our approach on a wide variety of models, datasets, and tasks -- specifically highlighting its use for Graph Neural Networks. So far, obtaining provable guarantees for GNNs has been difficult due to the discrete and non-i.i.d. nature of graph data. Our method can certify any GNN and handles perturbations to both the graph structure and the node attributes.},
  code      = {https://github.com/abojchevski/sparse_smoothing},
  arxiv     = {2008.12952},
  abbr      = {ICML},
  category  = {conference},
  selected  = {true},
  slides    = {slides_sparse_smoothing.pdf},
  talk      = {https://icml.cc/virtual/2020/poster/6848},
}
@inproceedings{bojchevski2020pprgo,
  title     = {Scaling Graph Neural Networks with Approximate {PageRank}},
  author    = {Bojchevski*, Aleksandar and Klicpera*, Johannes and Perozzi, Bryan and Kapoor, Amol and Blais, Martin and R{\'o}zemberczki, Benedek and Lukasik, Michal and G{\"u}nnemann, Stephan},
  booktitle = {International Conference on Knowledge Discovery and Data Mining, {KDD}},
  year      = {2020},
  abstract  = {Graph neural networks (GNNs) have emerged as a powerful approach for solving many network mining tasks. However, learning on large graphs remains a challenge - many recently proposed scalable GNN approaches rely on an expensive message-passing procedure to propagate information through the graph. We present the PPRGo model which utilizes an efficient approximation of information diffusion in GNNs resulting in significant speed gains while maintaining state-of-the-art prediction performance. In addition to being faster, PPRGo is inherently scalable, and can be trivially parallelized for large datasets like those found in industry settings. We demonstrate that PPRGo outperforms baselines in both distributed and single-machine training environments on a number of commonly used academic graphs. To better analyze the scalability of large-scale graph learning methods, we introduce a novel benchmark graph with 12.4 million nodes, 173 million edges, and 2.8 million node features. We show that training PPRGo from scratch and predicting labels for all nodes in this graph takes under 2 minutes on a single machine, far outpacing other baselines on the same graph. We discuss the practical application of PPRGo to solve large-scale node classification problems at Google.},
  abbr      = {KDD},
  category  = {conference},
  code      = {https://github.com/TUM-DAML/pprgo_pytorch},
  arxiv     = {2007.01570},
  award     = {Oral},
  slides    = {slides_pprgo.pdf},
  talk      = {https://www.youtube.com/watch?v=nsngoAycymE},
}
@inproceedings{angriman2020group,
  author    = {Eugenio Angriman and
               Alexander van der Grinten and
               Aleksandar Bojchevski and
               Daniel Z{\"{u}}gner and
               Stephan G{\"{u}}nnemann and
               Henning Meyerhenke},
  title     = {Group Centrality Maximization for Large-scale Graphs},
  booktitle = {Symposium on Algorithm Engineering and Experiments, {ALENEX}},
  year      = {2020},
  abstract  = {The study of vertex centrality measures is a key aspect of network analysis. Naturally, such centrality measures have been generalized to groups of vertices; for popular measures it was shown that the problem of finding the most central group is NP-hard. As a result, approximation algorithms to maximize group centralities were introduced recently. Despite a nearly-linear running time, approximation algorithms for group betweenness and (to a lesser extent) group closeness are rather slow on large networks due to high constant overheads. That is why we introduce GED-Walk centrality, a new submodular group centrality measure inspired by Katz centrality. In contrast to closeness and betweenness, it considers walks of any length rather than shortest paths, with shorter walks having a higher contribution. We define algorithms that (i) efficiently approximate the GED-Walk score of a given group and (ii) efficiently approximate the (proved to be NP-hard) problem of finding a group with highest GED-Walk score. Experiments on several real-world datasets show that scores obtained by GED-Walk improve performance on common graph mining tasks such as collective classification and graph-level classification. An evaluation of empirical running times demonstrates that maximizing GED-Walk (in approximation) is two orders of magnitude faster compared to group betweenness approximation and for group sizes ≤100 one to two orders faster than group closeness approximation. For graphs with tens of millions of edges, approximate GED-Walk maximization typically needs less than one minute. Furthermore, our experiments suggest that the maximization algorithms scale linearly with the size of the input graph and the size of the group.},
  abbr      = {ALENEX},
  category  = {conference},
  arxiv     = {1910.13874},
  code      = {https://networkit.github.io/}
}
@inproceedings{bojchevski19certifiable,
  author    = {Aleksandar Bojchevski and
               Stephan G{\"{u}}nnemann},
  title     = {Certifiable Robustness to Graph Perturbations},
  booktitle = {Neural Information Processing Systems, {NeurIPS}},
  year      = {2019},
  abstract  = {Despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness. This is even more alarming given recent findings showing that they are extremely vulnerable to adversarial attacks on both the graph structure and the node attributes. We propose the first method for verifying certifiable (non-)robustness to graph perturbations for a general class of models that includes graph neural networks and label/feature propagation. By exploiting connections to PageRank and Markov decision processes our certificates can be efficiently (and under many threat models exactly) computed. Furthermore, we investigate robust training procedures that increase the number of certifiably robust nodes while maintaining or improving the clean predictive accuracy.},
  selected  = {true},
  abbr      = {NeurIPS},
  category  = {conference},
  arxiv     = {1910.14356},
  code      = {https://github.com/abojchevski/graph_cert},
  poster    = {poster_graph_cert.pdf}
}
@inproceedings{bojchevski2019adversarial,
  title     = {Adversarial Attacks on Node Embeddings via Graph Poisoning},
  author    = {Aleksandar Bojchevski and Stephan G{\"{u}}nnemann},
  booktitle = {International Conference on Machine Learning, {ICML}},
  year      = {2019},
  abstract  = {The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for solving downstream tasks. However, despite the proliferation of such methods, there is currently no study of their robustness to adversarial attacks. We provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial perturbations that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.},
  abbr      = {ICML},
  category  = {conference},
  arxiv     = {1809.01093},
  code      = {https://github.com/abojchevski/node_embedding_attack},
  award     = {Oral},
  selected  = {true},
  slides    = {slides_embedding_attack.pdf},
  poster    = {poster_embedding_attack.pdf},
  talk      = {https://www.videoken.com/embed/1zMVZKlxfU4?tocitem=1},
}
@inproceedings{klicpera19predict,
  author    = {Johannes Klicpera and
               Aleksandar Bojchevski and
               Stephan G{\"{u}}nnemann},
  title     = {Predict then Propagate: Graph Neural Networks meet Personalized {PageRank}},
  booktitle = {International Conference on Learning Representations, {ICLR}},
  year      = {2019},
  abstract  = {Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.},
  abbr      = {ICLR},
  category  = {conference},
  arxiv     = {1810.05997},
  code      = {https://github.com/klicperajo/ppnp},
  poster    = {poster_ppnp.pdf}
}
@inproceedings{bojchevski2019pagerank,
  title     = {Is {PageRank} All You Need for Scalable Graph Neural Networks?},
  author    = {Bojchevski, Aleksandar and Klicpera, Johannes and Perozzi, Bryan and Blais, Martin and Kapoor, Amol and Lukasik, Michal and G{\"u}nnemann, Stephan},
  booktitle = {International Workshop on Mining and Learning with Graphs, MLG},
  year      = {2019},
  abstract  = {Graph neural networks (GNNs) have emerged as a powerful approach for solving many network mining tasks. However, efficiently utilizing them on web-scale data remains a challenge despite related advances in research. Most recently proposed scalable GNNs rely on an expensive recursive message-passing procedure to propagate information through the graph. We circumvent this limitation by leveraging connections between GNNs and personalized PageRank and we develop a model that incorporates multi-hop neighborhood information in a single (non-recursive) step. Our work-in-progress approach PPRGo is significantly faster than multi-hop models while maintaining state-of-the-art prediction performance. We demonstrate the strengths and scalability of our approach on graphs orders of magnitude larger than typically considered in the literature.},
  abbr      = {MLG},
  category  = {workshop},
  code      = {https://github.com/TUM-DAML/pprgo_pytorch},
  pdf       = {https://www.mlgworkshop.org/2019/papers/MLG2019_paper_50.pdf}
}
@inproceedings{monti2018dual,
  title     = {Dual-primal Graph Convolutional Networks},
  author    = {Monti, Federico and Shchur, Oleksandr and Bojchevski, Aleksandar and Litany, Or and G{\"u}nnemann, Stephan and Bronstein, Michael M},
  booktitle = {Graph Embedding and Mining Workshop, {GEM}},
  year      = {2019},
  abstract  = {In recent years, there has been a surge of interest in developing deep learning methods for non-Euclidean structured data such as graphs. In this paper, we propose Dual-Primal Graph CNN, a graph convolutional architecture that alternates convolution-like operations on the graph and its dual. Our approach allows to learn both vertex- and edge features and generalizes the previous graph attention (GAT) model. We provide extensive experimental validation showing state-of-the-art results on a variety of tasks tested on established graph benchmarks, including CORA and Citeseer citation networks as well as MovieLens, Flixter, Douban and Yahoo Music graph-guided recommender systems.},
  abbr      = {GEM},
  category  = {workshop},
  arxiv     = {1806.00770}
}
@inproceedings{bojchevski2018netgan,
  author    = {Aleksandar Bojchevski* and
               Oleksandr Shchur* and
               Daniel Z{\"{u}}gner* and
               Stephan G{\"{u}}nnemann},
  title     = {{NetGAN}: Generating Graphs via Random Walks},
  booktitle = {International Conference on Machine Learning, {ICML}},
  year      = {2018},
  abstract  = {We propose NetGAN -- the first implicit generative model for graphs able to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over the input graph. The proposed model is based on a stochastic neural network that generates discrete output samples and is trained using the Wasserstein GAN objective. NetGAN is able to produce graphs that exhibit well-known network patterns without explicitly specifying them in the model definition. At the same time, our model exhibits strong generalization properties, as highlighted by its competitive link prediction performance, despite not being trained specifically for this task. Being the first approach to combine both of these desirable properties, NetGAN opens exciting avenues for further research.},
  abbr      = {ICML},
  category  = {conference},
  arxiv     = {1803.00816},
  code      = {https://github.com/danielzuegner/netgan},
  award     = {Oral},
  slides    = {slides_netgan.pdf},
  poster    = {poster_netgan.pdf}
}
@inproceedings{bojchevski2018deep,
  title     = {Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking},
  author    = {Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
  booktitle = {International Conference on Learning Representations, {ICLR}},
  year      = {2018},
  abstract  = {Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss -- an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty -- by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.},
  abbr      = {ICLR},
  category  = {conference},
  arxiv     = {1707.03815},
  code      = {https://github.com/abojchevski/graph2gauss},
  poster    = {poster_graph2gauss.pdf}
}
@inproceedings{bojchevski2018bayesian,
  title     = {Bayesian Robust Attributed Graph Clustering: Joint Learning of Partial Anomalies and Group Structure},
  author    = {Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
  booktitle = {Conference on Artificial Intelligence, {AAAI}},
  year      = {2018},
  abstract  = {We study the problem of robust attributed graph clustering. In real data, the clustering structure is often obfuscated  due to anomalies or corruptions. While robust methods have been recently introduced that handle anomalies as part of the clustering process, they all fail to account for one core aspect: 	Since attributed graphs consist of two views (network structure and attributes) anomalies might materialize only partially, i.e. instances might be corrupted in one view but perfectly fit in the other. In this case, we can still derive meaningful cluster assignments. Existing works only consider complete anomalies. In this paper, we present a novel probabilistic generative model (PAICAN) that explicitly models partial anomalies by generalizing ideas of Degree Corrected Stochastic Block Models and Bernoulli Mixture Models. We provide a highly scalable variational inference approach with runtime complexity linear in the number of edges. The robustness of our model w.r.t. anomalies is demonstrated by our experimental study, outperforming state-of-the-art competitors.},
  abbr      = {AAAI},
  category  = {conference},
  code      = {https://github.com/abojchevski/paican},
  poster    = {poster_paican.pdf},
  pdf       = {paper_paican.pdf},
}
% html      = {https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16363},
@inproceedings{shchur2018pitfalls,
  title     = {Pitfalls of Graph Neural Network Evaluation},
  author    = {Shchur, Oleksandr and Mumme, Maximilian and Bojchevski, Aleksandar and G{\"u}nnemann, Stephan},
  booktitle = {Relational Representation Learning Workshop, R2L},
  year      = {2018},
  abstract  = {Semi-supervised node classification in graphs is a fundamental problem in graph mining, and the recently proposed graph neural networks (GNNs) have achieved unparalleled results on this task. Due to their massive success, GNNs have attracted a lot of attention, and many novel architectures have been put forward. In this paper we show that existing evaluation strategies for GNN models have serious shortcomings. We show that using the same train/validation/test splits of the same datasets, as well as making significant changes to the training procedure (e.g. early stopping criteria) precludes a fair comparison of different architectures. We perform a thorough empirical evaluation of four prominent GNN models and show that considering different splits of the data leads to dramatically different rankings of models. Even more importantly, our findings suggest that simpler GNN architectures are able to outperform the more sophisticated ones if the hyperparameters and the training procedure are tuned fairly for all models.},
  abbr      = {R2L},
  category  = {workshop},
  arxiv     = {1811.05868},
  code      = {https://github.com/shchur/gnn-benchmark}
}@inproceedings{shchur2018anomaly,
  author    = {Oleksandr Shchur and
               Aleksandar Bojchevski and
               Mohamed Farghal and
               Stephan G{\"{u}}nnemann and
               Yusuf Saber},
  title     = {Anomaly Detection in Car-Booking Graphs},
  booktitle = {International Conference on Data Mining Workshops, {ICDM}},
  year      = {2018},
  abstract  = {The use of car-booking services has gained massive popularity in the recent years -- which led to an increasing number of fraudsters that try to game these systems. In this paper we describe a framework for fraud detection in car-booking systems. Our core idea lies in casting this problem as an instance of anomaly detection in temporal graphs. Specifically, we use unsupervised techniques, such as dense subblock discovery, to detect suspicious activity. The proposed framework is able to adapt to the variations in the data inherent to the car-booking setting, and detects fraud with high precision. This work is performed in collaboration with Careem, where the described framework is currently being deployed in production.},
  abbr      = {ICDMW},
  category  = {workshop},
  html      = {https://ieeexplore.ieee.org/document/8637361}
}
@article{cejuela2018loctext,
  title    = {LocText: Relation Extraction of Protein Localizations to Assist Database Curation},
  author   = {Cejuela, Juan Miguel and Vinchurkar, Shrikant and Goldberg, Tatyana and Shankar, Madhukar S. Prabhu and Baghudana, Ashish and Bojchevski, Aleksandar and Uhlig, Carsten and Ofner, Andr{\'e} and Raharja-Liu, Pandu and Jensen, Lars Juhl and others},
  journal  = {BMC Bioinformatics},
  year     = {2018},
  abstract = {The subcellular localization of a protein is an important aspect of its function. However, the experimental annotation of locations is not even complete for well-studied model organisms. Text mining might aid database curators to add experimental annotations from the scientific literature. Existing extraction methods have difficulties to distinguish relationships between proteins and cellular locations co-mentioned in the same sentence. LocText was created as a new method to extract protein locations from abstracts and full texts. LocText learned patterns from syntax parse trees and was trained and evaluated on a newly improved LocTextCorpus. Combined with an automatic named-entity recognizer, LocText achieved high precision (P = 86%±4). After completing development, we mined the latest research publications for three organisms: human (Homo sapiens), budding yeast (Saccharomyces cerevisiae), and thale cress (Arabidopsis thaliana). Examining 60 novel, text-mined annotations, we found that 65% (human), 85% (yeast), and 80% (cress) were correct. Of all validated annotations, 40% were completely novel, i.e. did neither appear in the annotations nor the text descriptions of Swiss-Prot. LocText provides a cost-effective, semi-automated workflow to assist database curators in identifying novel protein localization annotations. The annotations suggested through text-mining would be verified by experts to guarantee high-quality standards of manually-curated databases such as Swiss-Prot.},
  html     = {https://pubmed.ncbi.nlm.nih.gov/29343218/},
  code     = {https://github.com/Rostlab/LocText},
  abbr     = {BMC},
  category = {conference}
}
@inproceedings{bojchevski2017robust,
  title     = {Robust Spectral Clustering for Noisy Data: Modeling Sparse Corruptions Improves Latent Embeddings},
  author    = {Bojchevski, Aleksandar and Matkovic, Yves and G{\"u}nnemann, Stephan},
  booktitle = {International Conference on Knowledge Discovery and Data Mining, {KDD}},
  year      = {2017},
  abstract  = {Spectral clustering is one of the most prominent clustering approaches. However, it is highly sensitive to noisy input data. In this work, we propose a robust spectral clustering technique able to handle such scenarios. To achieve this goal, we propose a sparse and latent decomposition of the similarity graph used in spectral clustering. In our model, we jointly learn the spectral embedding as well as the corrupted data -- thus, enhancing the clustering performance overall. We propose algorithmic solutions to all three established variants of spectral clustering, each showing linear complexity in the number of edges. Our experimental analysis confirms the significant potential of our approach for robust spectral clustering. Supplementary material is available at www.kdd.in.tum.de/RSC.},
  abbr      = {KDD},
  category  = {conference},
  html      = {https://dl.acm.org/doi/10.1145/3097983.3098156},
  code      = {https://github.com/abojchevski/rsc},
  poster    = {poster_rsc.pdf},
  pdf       = {paper_rsc.pdf},
}
@article{cejuela2017nala,
  title    = {nala: Text Mining Natural Language Mutation Mentions},
  author   = {Cejuela, Juan Miguel and Bojchevski, Aleksandar and Uhlig, Carsten and Bekmukhametov, Rustem and Kumar Karn, Sanjeev and Mahmuti, Shpend and Baghudana, Ashish and Dubey, Ankit and Satagopam, Venkata P and Rost, Burkhard},
  journal  = {Bioinformatics},
  year     = {2017},
  abstract = {The extraction of sequence variants from the literature remains an important task. Existing methods primarily target standard (ST) mutation mentions (e.g. ‘E6V’), leaving relevant mentions natural language (NL) largely untapped (e.g. ‘glutamic acid was substituted by valine at residue 6’). We introduced three new corpora suggesting named-entity recognition (NER) to be more challenging than anticipated: 28–77% of all articles contained mentions only available in NL. Our new method nala captured NL and ST by combining conditional random fields with word embedding features learned unsupervised from the entire PubMed. In our hands, nala substantially outperformed the state-of-the-art. For instance, we compared all unique mentions in new discoveries correctly detected by any of three methods (SETH, tmVar, or nala). Neither SETH nor tmVar discovered anything missed by nala, while nala uniquely tagged 33% mentions. For NL mentions the corresponding value shot up to 100% nala-only.},
  html     = {https://academic.oup.com/bioinformatics/article/33/12/1852/2991428},
  code     = {https://github.com/Rostlab/nala},
  abbr     = {BioInf},
  category = {conference}
}

@thesis{bojchevski2020thesis,
  title    = {Machine Learning on Graphs in the Presence of Noise and Adversaries},
  author   = {Bojchevski, Aleksandar},
  year     = {2020},
  abstract = {From protein interactions to social networks, complex systems of interlinked entities are endemic in a connected world and graphs are a powerful abstraction for capturing their structure. Accordingly, we have a rich literature of machine learning techniques for graph data to solve problems ranging from fraud detection to cancer classification. Since in reality data is unreliable understanding the robustness of these techniques to noise and adversaries is critical. The contributions of this thesis deepen our understanding of robustness for three types of models: unsupervised, generative, and semi-supervised.  First, we derive a noise-resilient variant of the classical spectral embedding, and we introduce Gaussian embeddings that represent nodes as distributions to capture uncertainty. Then we study the sensitivity of node embeddings to graph poisoning. Next, we develop a generative model that explicitly accounts for anomalies and detects clusters obfuscated by noise. Finally, we derive provable robustness guarantees. We propose the first certificate w.r.t. structure perturbations for a large class of PageRank-based models, and we derive a general certificate for discrete data applicable to any graph classifier.},
  school   = {Technical University of Munich},
  category = {thesis},
  abbr     = {PhD},
}

@thesis{bojchevski2016thesis,
  title    = {Semi-supervised Learning for Biomedical Named-Entity Recognition},
  author   = {Bojchevski, Aleksandar},
  year     = {2015},
  abstract = {As the volume of published research in the biomedical domain increases, the need for effective information extraction systems grows in parallel. In this context, the task of named-entity recognition (NER) is essential. NER is defined as the classification of words in free text that represent predefined categories such as genes, proteins or other entities. As a specific application of NER, the main focus of this thesis is the recognition of mutation mentions from the biomedical literature. More specifically we aim to create a model able to recognize mutation mentions expressed in natural language. The current state-of-the-art method, tmVar is only able to recognize a small subset of standard or semi-standard mentions. Our method both outperforms tmVar on those types of mentions and is also able to recognize natural language (NL) mentions. Previously no other method considered NL mutation mentions. The performance of NER machine learning models is intrinsically limited by the availability of high-quality-annotated corpora. The construction of such corpora is costly -- specially when expert annotators are required. In the biomedical domain, the difficulty of the task is even greater, since the number of possible named entities is higher and keeps growing with new discoveries. To combat the lack of large annotated corpora, we turn to the exploitation of large volumes of unlabeled text, applying a semi-supervised learning approach. Using techniques for unsupervised feature learning we aim to increase the performance of traditional NER models. More specifically, this thesis focuses on augmenting common conditional random field (CRF) approaches combined with novel word representation features learned from large bodies of biomedical text. Furthermore, using an active learning approach we extend an existing corpus of mutation mentions (IDP4) with additional NL mentions. Finally, and in support of evaluating our semi-supervised learning approach, we develop a complete pipeline for biomedical named-entity recognition including preprocessing steps, feature generation, model learning and normalized predictions. Our extended corpus, NER tool and pipeline framework are all open sourced on GitHub.},
  school   = {Technical University of Munich},
  category = {thesis},
  pdf      = {master_thesis.pdf},
  abbr     = {MSc},
  slides   = {master_thesis_slides.pdf},
  code     = {https://github.com/Rostlab/nala}
}

@thesis{bojchevski2013,
  title    = {Personality Prediction Based on Information from Social Networks},
  author   = {Bojchevski, Aleksandar},
  year     = {2013},
  school   = {Ss. Cyril and Methodius University},
  category = {thesis},
  abbr     = {BEng},
}


